name: Tests

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: ["3.12"]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        enable-cache: true
        cache-dependency-glob: "uv.lock"
    
    - name: Set up Python ${{ matrix.python-version }}
      run: uv python install ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: uv sync
    
    - name: Start FastAPI server
      run: |
        uv run uvicorn src.main:app --port 8000 &
        echo $! > server.pid
        sleep 5  # Wait for server to start
    
    - name: Run tests (without LLM)
      run: uv run pytest tests/ -v -k "not lm_studio"
    
    - name: Run LLM tests with OpenAI
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        LLM_API_URL: "https://api.openai.com/v1/chat/completions"
        LLM_MODEL: "gpt-3.5-turbo"
      run: |
        if [ -n "$OPENAI_API_KEY" ]; then
          echo "Running LLM tests with OpenAI API"
          uv run pytest tests/test_both.py::test_lm_studio -v -s
        else
          echo "Skipping LLM tests (no OpenAI API key)"
        fi
    
    - name: Stop server
      if: always()
      run: |
        if [ -f server.pid ]; then
          kill $(cat server.pid) || true
          rm server.pid
        fi